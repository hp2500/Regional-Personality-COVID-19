---
title: "COVID19"
author: "Heinrich Peters"
date: "4/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# MAC
 knitr::opts_knit$set(root.dir = '/Users/hp2500/Google Drive/STUDY/Columbia/Research/Corona/Data')
 
 library(tidyverse)
 library(lmerTest)
 library(nlme)
 library(psych)

```

# Import Data
```{r, warning=FALSE, message=FALSE}

df <- read_csv('mp_features.csv')
df %>% head()

df_pers <- read_csv('gps_study_questionnares.csv')
df_pers %>% head()

df_home <- read_csv('home2.csv')
df_home %>% head()

```

# Prepare GPS Data

### Slice data frame
```{r}

# drop pandas index
df = df %>% select(-X1) 

# filter to march 2020
df <- df %>% filter(date > '2020-02-29')

# cast uid as factor
df <- df %>% mutate(uid = as.factor(uid))

# mobility features 
gps_vars <- c('raw_ent_value', 'num_cluster_value', 'per_at_home', 
              'transition_time_value', 'dis_value', 'convex_hull_value')

# picked raw entropy, too many missing in entropy
df <- df %>% select(uid, date, raw_ent_value, all_of(gps_vars), num_gps_records)

```


### Check data density
```{r}

# check missing values
colSums(is.na(df))

# check records per day
df$num_gps_records %>% 
  hist(main='GPS records per day')

# check records per person
df %>% group_by(uid) %>% 
  summarise(num_gps_records = sum(num_gps_records)) %>% 
  .$num_gps_records %>% 
  hist(main='GPS records per person')

# check days per person
df %>% group_by(uid) %>% 
  summarise(days = n()) %>% 
  .$days %>% 
  hist(main = 'Days per person')

```

### Drop incomplete entries
```{r}
# remove missing data 
df_clean_1 <- df %>% drop_na()
```

### Drop days with few observations
```{r}
# plot proportion dropped vs threshold
days_dropped <- NULL
for(i in seq(0, 1000)){
  dd_temp <- df_clean_1 %>%  filter(num_gps_records < i) %>% nrow()
  days_dropped <- c(days_dropped, dd_temp)
}

plot(days_dropped/nrow(df_clean_1), xlab= "Threshold", ylab = 'Proportion dropped')

# remove days with few observations
df_clean_2 <- df_clean_1 %>% filter(num_gps_records > 20)
```

### Multivariate outlier detection
```{r}
# calculate mahalanobis distances
mahal_dist <- df_clean_2 %>% 
  select(all_of(gps_vars)) %>%
  mahalanobis(center = colMeans(.), cov = cov(.))

# calculate chisq cutoff
cutoff = qchisq(0.999, 6)
sum(mahal_dist > cutoff)

# drop atypical days
df_clean_3 = df_clean_2[mahal_dist < cutoff,]
```

### Drop people with few days
```{r}
# remove people with few days
df_clean_4 <- df_clean_3 %>%
  group_by(uid) %>%
  filter(n() >=10) %>%
  ungroup()
```


### Check how much data was dropped
```{r}

# how many observations
(nrow(df) - nrow(df_clean_4)) / nrow(df)

# how many people
(length(unique(df$uid)) - length(unique(df_clean_4$uid))) / length(unique(df$uid))

# cleaned df
df_clean <- df_clean_4

```

### Double check data density
```{r}
df_clean$num_gps_records %>% 
  hist(main='GPS records per day')

df_clean %>% group_by(uid) %>% 
  summarise(num_gps_records = sum(num_gps_records)) %>% 
  .$num_gps_records %>%
  hist(main='GPS records per person')

df_clean %>% group_by(uid) %>% 
  summarise(days = n()) %>% 
  .$days %>% 
  hist(main = 'Days per person')
```


### Format time variable
```{r}

# create sequence of dates
date_sequence <- seq.Date(min(as.Date(df$date)),
                     max(as.Date(df$date)), 1)

# create data frame with time sequence
df_dates = tibble(date_sequence, 1:length(date_sequence)) 
names(df_dates) <- c('date', 'day')

# merge day index with gps data
df_clean = df_clean %>% 
  merge(df_dates, by='date') %>% 
  arrange(uid) %>%
  as_tibble()

df_clean %>% head()

```


### Calculate social distancing index
```{r}
# calculate index score
df_scaled <- df_clean %>%
  mutate_at(vars(all_of(gps_vars)), scale) %>%
  mutate(per_at_home= -per_at_home) %>%
  rowwise() %>%
  mutate(SDI = - mean(c(raw_ent_value:convex_hull_value)))
```

### Run PCA for comparison
```{r}
# calculate pca score
pca_pc1 <- df_clean %>% 
  select(all_of(gps_vars)) %>% 
  pca(nfactors = 1) %>% 
  .$score

df_scaled <- df_scaled %>% cbind(-pca_pc1)

# check correlation
cor(df_scaled$SDI, df_scaled$PC1)
```

### Check Cronbachs alpha
```{r}
# calculate cronbachs alpha
df_scaled %>% 
  select(all_of(gps_vars)) %>% 
  psych::alpha() %>%
  .$total

```


# Prepare personality data

### Drop irrelevant data
```{r}
# select relevant variables
df_pers_clean <- df_pers %>% 
  mutate(identity_id=as.factor(identity_id)) %>%
  select(identity_id, anxious_calm, critical_sympathetic, 
         dependable_disorganized, extroverted_reserved, 
         open_conventional)
```


### Rename variables
```{r}
# rename variables
df_pers_clean <- df_pers_clean %>%  
  rename(uid=identity_id, 
         n_pers=anxious_calm,
         a_pers=critical_sympathetic,
         c_pers=dependable_disorganized,
         e_pers=extroverted_reserved,
         o_pers=open_conventional)

```

### Recode personality scores
```{r}

# recode inverted scores
df_pers_clean <- df_pers_clean %>% 
  mutate(e_pers = 6-e_pers,
         c_pers = 6-c_pers,
         n_pers = 6-n_pers,
         o_pers = 6-o_pers)

```



### Check distributions 
```{r}
# histograms
df_pers_clean %>% select(-uid) %>% map(hist, breaks=0:6, main="", xlab="") %>% invisible()

```

# Prepare home state data
```{r, warning=FALSE}

# Extract states from address strings
df_home_clean <- df_home %>% 
  separate(address, into = c('city', 'state', NULL), sep=",") %>% 
  mutate(state = str_extract(state, '[A-Z]{2}')) %>%
  rename(uid = identity_id) %>%
  select(uid, state) %>% 
  filter(state %in% state.abb)

```

# Merge data 
```{r}
# inner join all relevant data frames
df_merged <- plyr::join_all(list(df_scaled, df_pers_clean, df_home_clean), by = 'uid', type = 'inner') %>%
  as_tibble()

df_merged 
```

# Analysis 

## Exploratory Analyses
### Check correlations
```{r}
df_merged %>% select(-uid, -date, -state) %>% cor()

```

### Check state data 

```{r}
# Check number of participants per state
df_merged %>% group_by(state) %>% 
  summarise(state_n = length(unique(uid))) %>% 
  arrange(desc(state_n))

# frequency distribution 
df_merged %>% group_by(state) %>% 
  summarise(state_n = length(unique(uid))) %>% 
  .$state_n %>% 
  hist(main= "Distribution of number of participants per state")

```



## Multilevel time series models

### Prepare data
```{r}
# scale data
df_merged <- df_merged %>% 
  select(uid, day, o_pers, c_pers, e_pers, a_pers, n_pers, SDI, state) %>% 
  mutate_at(vars(-uid, -day, -SDI, -state), scale)
```


### Create functions to run all models 
```{r}

# function calculates all relevant models
run_models <- function(pers, sdi = 'SDI', data){

  # subset data
  data = data %>% 
    select(uid, day, sdi, all_of(pers), state) %>% 
    rename(pers = all_of(pers),
           sdi = all_of(sdi)
           )

    # baseline
  baseline <- lme(fixed = sdi ~ 1, random = ~ 1 | uid, 
                    data = data,
                    correlation = corAR1())

  # random intercept fixed slope
  random_intercept <- lme(fixed = sdi ~ day + pers , random = ~ 1 | uid,
                            data = data,
                            correlation = corAR1())

  # random intercept random slope
  random_slope <- lme(fixed = sdi ~ day + pers, random = ~ day | uid, 
                        data = data,
                        correlation = corAR1())

  # cross level interaction
  interaction <- lme(fixed = sdi ~ day * pers, random = ~ day | uid, 
                       data = data,
                       correlation = corAR1())

  # baseline
  baseline_sfe <- lme(fixed = sdi ~ state, random = ~ 1 | uid, 
                    data = data,
                    correlation = corAR1())

  # random intercept fixed slope
  random_intercept_sfe <- lme(fixed = sdi ~ day + pers + state, random = ~ 1 | uid,
                            data = data,
                            correlation = corAR1())

  # random intercept random slope
  random_slope_sfe <- lme(fixed = sdi ~ day + pers + state, random = ~ day | uid, 
                        data = data,
                        correlation = corAR1())

  # cross level interaction
  interaction_sfe <- lme(fixed = sdi ~ day * pers + state, random = ~ day | uid, 
                       data = data,
                       correlation = corAR1())
  
  # create list with results
  results <- list('baseline' = baseline, 
                  "random_intercept" = random_intercept, 
                  "random_slope" = random_slope,
                  "interaction" = interaction,
                  "baseline_state_fixed" = baseline_sfe, 
                  "random_intercept_state_fixed" = random_intercept_sfe, 
                  "random_slope_state_fixed" = random_slope_sfe,
                  "interaction_state_fixed" = interaction_sfe)
  
  return(results)
        
}


# extracts table with coefficients and tests statistics
extract_results <- function(models) {
  
  models_summary <- models %>% 
  map(summary) %>% 
  map("tTable") %>% 
  map(as.data.frame) %>% 
  map(round, 5) %>% 
  map(~ .[str_detect(rownames(.), 'Inter|day|pers'),])
  
  return(models_summary)
}

```

### Openness
```{r}

models_o <- run_models(pers = 'o_pers', sdi = 'SDI', data = df_merged)
models_o %>% extract_results() %>% print()


```

### Conscientiousness
```{r}

models_c <- run_models(pers = 'c_pers', sdi = 'SDI', data = df_merged)
models_c %>% extract_results() %>% print()

```

### Extraversion

```{r}

models_e <- run_models(pers = 'e_pers', sdi = 'SDI', data = df_merged)
models_e %>% extract_results() %>% print()

```


### Agreeableness
```{r}

models_a <- run_models(pers = 'a_pers', sdi = 'SDI', data = df_merged)
models_a %>% extract_results() %>% print()

```


### Neuroticism 
```{r}

models_n <- run_models(pers = 'n_pers', sdi = 'SDI', data = df_merged)
models_n %>% extract_results() %>% print()

```

